# @package _global_
exp_name: sft_qwen2_vl-2b_inst

# Model arguments
model_name_or_path: Qwen/Qwen2-VL-2B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2
min_pixels: 3136
max_pixels: 501760
freeze_llm: false
freeze_vision: false

# Lora Arguments
# No LoRA is used here

# Data training arguments
dataset_name: 
  MMInstruction/Clevr_CoGenT_TrainA_R1: 1.0
dataset_splits:
  - train
dataset_configs:
  - default
system_prompt: "A conversation between User and Assistant. The user asks a question about the image, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\nUser: {problem} \nAssistant: Let me solve this step by step.\n"
truncation_side: left
padding_side: left
preprocessing_num_workers: 5
wandb_project: bmir
use_unsloth: false
unsloth_configs:
  max_seq_length: 2048
  load_in_4bit: true
  fast_inference: true
  gpu_memory_utilization: 0.8
  trust_remote_code: true

# SFT trainer config
output_dir: outputs/Qwen2-VL-2B-Instruct-SFT
overwrite_output_dir: true
bf16: true
max_steps: -1
num_train_epochs: 1
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
do_eval: true
eval_strategy: "no"
per_device_eval_batch_size: 4
learning_rate: 2.0e-05
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_seq_length: 4096
skip_prepare_dataset: true
packing: false

# Logging arguments
log_level: info
logging_steps: 5
logging_strategy: steps
report_to:
  - wandb
save_strategy: "no"
seed: 42
# wandb_entity: sft
# wandb_project: sft_qwen2_vl-2b_inst

# Hugging Face Hub
push_to_hub: false
hub_model_id: Qwen2-VL-2B-Instruct-SFT
hub_strategy: every_save